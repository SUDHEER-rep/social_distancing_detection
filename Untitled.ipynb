{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial import distance as dist\n",
    "import argparse\n",
    "import imutils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/\"\n",
    "\n",
    "#input and output files\n",
    "output_path =  \"output.avi\"\n",
    "input_path = \"videoplayback\"\n",
    "\n",
    "# # load the COCO class labels our YOLO model was trained on\n",
    "labelsPath =  \"coco.names.txt\"\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "# # derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = \"yolov3.weights\"\n",
    "configPath =\"yolo3.cfg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Initialize Variables for configuration\n",
    "\n",
    "# initialize minimum probability to filter weak detections along with\n",
    "# the threshold when applying non-maxima suppression\n",
    "MIN_CONF = 0.3\n",
    "NMS_THRESH = 0.3\n",
    "\n",
    "# # boolean indicating if NVIDIA CUDA GPU should be used. Kept this False as I didnt wanna get into setting up the GPU requirements on colab.\n",
    "# USE_GPU = False\n",
    "\n",
    "#Min Distance to be observed while social distancing in pixels. Anything above this would be marked as a violation.\n",
    "MIN_DISTANCE = 40\n",
    "\n",
    "display = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, model, outputL_names, person_idx = 0):\n",
    "  (H, W) = frame.shape[:2]\n",
    "  results = []\n",
    "\n",
    "  boxes = []\n",
    "  centroids = []\n",
    "  confidences = []\n",
    "  #Creating a blob and performing forwards pass.\n",
    "  blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),swapRB=True, crop=False)\n",
    "  model.setInput(blob)\n",
    "  layerOutputs = model.forward(outputL_names)\n",
    "  for output in layerOutputs:\n",
    "    for detection in output:\n",
    "      scores = detection[5:]\n",
    "      classID = np.argmax(scores)\n",
    "      confidence = scores[classID]\n",
    "      if classID == person_idx and confidence > MIN_CONF:\n",
    "        #scale our bounding box according to the frame size.\n",
    "        box = detection[0:4] * np.array([W, H, W, H])\n",
    "        #derive top-left corner\n",
    "        (cX, cY, width, height) = box.astype(\"int\")\n",
    "        blX = int(cX - (width/2))\n",
    "        blY = int(cY - (height/2))\n",
    "\n",
    "        #update our lists\n",
    "        boxes.append([blX, blY, int(width), int(height)])\n",
    "        centroids.append((cX, cY))\n",
    "        confidences.append(float(confidence))\n",
    "\n",
    "  #now lets apply non-maxima suppression\n",
    "  idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "  if len(idxs) > 0:\n",
    "    for idx in idxs.flatten():\n",
    "      (x, y) = (boxes[idx][0], boxes[idx][1])\n",
    "      (w, h) = (boxes[idx][2], boxes[idx][3])\n",
    "      results.append((confidences[idx], (x, y, x+w, y+h), centroids[idx]))\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading our YOLO model ....\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-h4wtvo23\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:207: error: (-212:Parsing error) Failed to parse NetParameter file: yolo3.cfg.txt in function 'cv::dnn::dnn4_v20200609::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-e7e786de38cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load our YOLO MODEL trained on COCO dataset from the drive.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading our YOLO model ....\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromDarknet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweightsPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-h4wtvo23\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:207: error: (-212:Parsing error) Failed to parse NetParameter file: yolo3.cfg.txt in function 'cv::dnn::dnn4_v20200609::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "#Load our YOLO MODEL trained on COCO dataset from the drive.\n",
    "print(\"Loading our YOLO model ....\")\n",
    "model = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ad65efecaffe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Determine the only output layer names we need from the YOLO model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutputL_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLayerNames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moutputL_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutputL_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetUnconnectedOutLayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#initialize our video stream and pointer to output file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Determine the only output layer names we need from the YOLO model.\n",
    "outputL_names = model.getLayerNames()\n",
    "outputL_names = [outputL_names[i[0] - 1] for i in model.getUnconnectedOutLayers()]\n",
    "\n",
    "#initialize our video stream and pointer to output file.\n",
    "print(\"Accessing the Video Stream ....\")\n",
    "vs = cv2.VideoCapture(input if input else 0)\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6d1eb2fc0684>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m   \u001b[1;33m(\u001b[0m\u001b[0mgrabbed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[1;31m#break if no frames grabbed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vs' is not defined"
     ]
    }
   ],
   "source": [
    "#Lets begin reading processing the video frames.\n",
    "\n",
    "while True:\n",
    "  (grabbed, frame) = vs.read()\n",
    "\n",
    "  #break if no frames grabbed\n",
    "  if not grabbed:\n",
    "    break\n",
    "  #scale\n",
    "  frame = imutils.resize(frame, width = 700)\n",
    "\n",
    "  #lets detect some people!\n",
    "  results = detect_people(frame, model, outputL_names, person_idx=LABELS.index(\"person\"))\n",
    "\n",
    "  #We will keep out Violations in a set\n",
    "  violations = set()\n",
    "  v_ = [[] for _ in range(len(results))]\n",
    "  #Start calculating pairwise distances of peple.\n",
    "  if len(results) >= 2:\n",
    "    centroids = np.array([r[2] for r in results])\n",
    "    #calculate Euclidean Distances between pairs of centroids\n",
    "    D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "    for i in range(0, D.shape[0]):\n",
    "      for j in range(i+1, D.shape[1]):\n",
    "        #check if the distance raises a violation\n",
    "        if D[i, j] < MIN_DISTANCE:\n",
    "          #Add the indexes to our set of violations\n",
    "          violations.add(i)\n",
    "          violations.add(j)\n",
    "          v_[i].append(j)\n",
    "    for (i , (conf, bbox, centroid)) in enumerate(results):\n",
    "      (startX, startY, endX, endY) = bbox\n",
    "      (cX, cY) = centroid\n",
    "      #set the default color of bounding boxes to green.\n",
    "      color = (0 , 255, 0)\n",
    "      #set the violation color of bounding boxes to red.\n",
    "      if i in violations:\n",
    "        color = (0, 0, 255)\n",
    "              #draw bboxes, centroids and lines b/w centroids\n",
    "      cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "      cv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "      for v in v_[i]:\n",
    "        (_, _, (c2X, c2Y)) = results[v]\n",
    "        cv2.line(frame, (cX, cY), (c2X, c2Y), color, 1)\n",
    "  text = \"Social Distancing Violations: {}\".format(len(violations))\n",
    "  cv2.putText(frame, text, (10, frame.shape[0] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255) if (len(violations) > 0) else (0, 255, 0), 3)\n",
    "\n",
    "  # check to see if the output frame should be displayed to our\n",
    "  # screen\n",
    "  if display > 0:\n",
    "    # show the output frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "      break\n",
    "  # if an output video file path has been supplied and the video\n",
    "  # writer has not been initialized, do so now\n",
    "  if output != \"\" and writer is None:\n",
    "    # initialize our video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    writer = cv2.VideoWriter(output, fourcc, 25,\n",
    "      (frame.shape[1], frame.shape[0]), True)\n",
    "  # if the video writer is not None, write the frame to the output\n",
    "  # video file\n",
    "  if writer is not None:\n",
    "    writer.write(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
